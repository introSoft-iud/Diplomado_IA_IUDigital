<!--  Nombre de la Unidad __
--># Unidad 1. Introducci√≥n a la construcci√≥n de aplicaciones con LLMs

## Introducci√≥n a la unidad

Bienvenidos a la primera unidad. En esta unidad aprender√°s, de manera general, c√≥mo funciona un modelo de lenguaje.  
Comenzaremos usando la API de OpenAI y exploraremos c√≥mo conectar sus servicios con nuestras aplicaciones.  
Luego aprender√°s a utilizar esta misma API a trav√©s del framework LangChain.

Finalmente, introduciremos los aspectos fundamentales de la interacci√≥n con los LLMs usando LangChain: **prompts**, **templates** y **output parsers**.

Como actividad pr√°ctica, elaborar√°s un sistema asistido por IA para extraer datos de comentarios de usuarios en un e-commerce.

¬°Comencemos!


<!--
Introducci√≥n a la unidad
Teniendo en cuenta que cada unidad es un saber espec√≠fico, en la introducci√≥n se destaca la importancia y relevancia del saber que se abordar√° en funci√≥n de los resultados de aprendizaje planteados. Describe brevemente c√≥mo el tema central de la unidad se integra en el panorama m√°s amplio del aprendizaje y de la vida cotidiana o profesional del estudiante. Su prop√≥sito es despertar la curiosidad y el inter√©s del estudiante sobre los contenidos que explorar√°.

En definitiva, se trata de responder a las preguntas: ¬øQu√© va a aprender el estudiante? ¬øC√≥mo lo va a aprender? ¬øPara qu√© lo va a aprender?  




Recomendaciones:
Inicia presentando al estudiante c√≥mo se relaciona el conocimiento de la unidad con su contexto. 
Incluye el prop√≥sito de la unidad y lo que el estudiante aprender√° mediante su estudio.
Vale la pena destacar algunos de los temas m√°s importantes que se abordar√°n.
Procura no superar las 300 palabras (1500 caracteres) al redactar la introducci√≥n.

-->

<!-- Resultados de aprendizaje
Los objetos de aprendizaje se asumen como aquello que los estudiantes ser√°n capaces de hacer a partir de lo que aprendieron a lo largo de la unidad.


Recomendaciones:
Formula m√°ximo dos resultados por unidad. 
Aseg√∫rate que tengan relaci√≥n con los objetivos de aprendizaje planteados en la carta descriptiva. 
Redacta los resultados a partir de tres elementos: qu√©, c√≥mo y para qu√©.
Recuerda que los resultados se establecen en funci√≥n del aprendizaje, no de la ense√±anza. 
Utiliza verbos conjugados en presente (describen la acci√≥n).  
Los resultados deben ser medibles y alcanzables. 




-->

## Resultados de aprendizaje


<!--Cronograma de actividades de la unidad  
Permite la proyecci√≥n de los contenidos tanto te√≥ricos como pr√°cticos, la ubicaci√≥n temporal dentro del curso y los porcentajes que corresponden a la evidencia de aprendizaje.


Recomendaciones:
Toma del cronograma general que realizaste (plantillas preliminares) las actividades que correspondan a la presente unidad.
Plantea una evidencia de aprendizaje por unidad, y otra m√°s para el cierre del curso. 

 -->

## Cronograma de actividades de la unidad  
**
## Cronograma de actividades - Unidad 1
| Actividad de aprendizaje       | Evidencia de aprendizaje | Semana       | Ponderaci√≥n |
|-------------------------------|---------------------------|--------------|--------------|
| Actividades de aprendizaje 1 y 2 | EA1                      | Semana 1 y 2 | 25%         |
| **Total**                      |                           |              | **25 %**     |
**


<!--
Desarrollo tem√°tico
Aqu√≠ comienza la elaboraci√≥n del contenido que har√° parte de la unidad tem√°tica. Para ello, es preciso identificar qu√© requiere el estudiante para aprender y comprender aquello que debe explorar, desarrollar nuevas habilidades, aplicar el conocimiento y cumplir con los resultados de aprendizaje. 

Los textos se construyen con cohesi√≥n y claridad, de manera que facilite al estudiante apropiarse del conocimiento de manera efectiva. Esta elaboraci√≥n debe estar respaldada por enfoques did√°cticos, garantizando un proceso de aprendizaje s√≥lido y bien fundamentado.




Recomendaciones:
Ten en cuenta las siguientes recomendaciones para desarrollar las tem√°ticas de la actividad de aprendizaje:
Lee el documento ‚ÄúManual del contenidista‚Äù en el cual encontrar√°s consejos para redactar los contenidos.
Ten a la mano el ‚ÄúManual de redacci√≥n‚Äù para resolver dudas o inquietudes sobre el uso de las normas APA para citas y referencias. 
Cada unidad debe tener una cantidad m√≠nima de 30 p√°ginas de contenido tem√°tico. Esto equivale aproximadamente a 8500 palabras, en Arial 12, espaciado 1.0 y texto justificado. 
El desarrollo del contenido requiere un 70 % de producci√≥n propia y un 30 % para contenidos de terceros (fuentes primarias). Monitorea permanentemente tu documento con ayuda de la herramienta Turnitin para revisar el porcentaje de similitud.
Los textos e im√°genes de terceros obligatoriamente se deben citar y referenciar, procurando que no superen el porcentaje exigido (30 %). Debes suministrar los enlaces de los recursos digitales empleados (PDF, sitios web, art√≠culos online, videos, im√°genes, etc.). Todos estos recursos deben ser de uso libre.
No incluir art√≠culos, tesis, textos o documentos propios que han sido previamente publicados o presentados a otra instituci√≥n. 
Los recursos como im√°genes, infograf√≠as, ilustraciones, tablas, etc., no hacen parte de las 30 p√°ginas del desarrollo de contenido.
Las figuras propias deben ser editables y se entregan en una carpeta aparte, cuidando que tengan el nombre y n√∫mero correspondiente. 
Las fuentes se pueden tomar de bases de datos de suscripci√≥n como EBSCO o de uso libre como Redalyc y Google Acad√©mico, las cuales cuentas con licencia Creative Commons (LCC) para su reproducci√≥n (solicitar el acceso a los repositorios en caso de no tenerlo).
Organiza y jerarquiza los temas y subtemas num√©ricamente.

-->
## ¬øQu√© es un modelo de lenguaje?

Un modelo de lenguaje es un sistema basado en *deep learning* que encapsula informaci√≥n sobre uno o varios lenguajes. Este sistema es entrenado para predecir qu√© tan probable es que una palabra aparezca en un determinado contexto.

Por ejemplo, dado el contexto:

> "Mi plato favorito es el ____"

un modelo de lenguaje que codifique el espa√±ol de Antioquia podr√≠a predecir "sancocho" con m√°s frecuencia que "ajiaco".
### Tokens

La unidad b√°sica de predicci√≥n de un modelo de lenguaje es el **token**, y el **tokenizador** es el software que utiliza el modelo para dividir los textos en tokens.

Por ejemplo, el tokenizador de GPT-4 divide la frase:

> "El sol est√° brillando intensamente"

de la siguiente manera:

<!--WARNING: El numerado autom√°tico de figuras no est√° funcionando. Arreglar-->

<figure>
  <img src="../assets/images/tokenizer.png" alt="Divisi√≥n en tokens de una frase en GPT-4" width="600">
  <figcaption>Divisi√≥n en tokens de una frase utilizando el tokenizador de GPT-4. Fuente: <a href="https://platform.openai.com/tokenizer">OpenAI Tokenizer</a>.</figcaption>
</figure>

!!! warning "Para tener en cuenta"
    Hay varias razones por las que los modelos de lenguaje utilizan **tokens** en lugar de palabras completas o caracteres individuales.

    A diferencia de un simple car√°cter, un token permite dividir una palabra en componentes con significado propio. Por ejemplo, la palabra **"intensamente"** puede ser dividida por el tokenizador en "intens" y "amente", y cada uno de estos componentes aporta parte del significado de la palabra completa.

    Esto tambi√©n implica que hay **menos tokens √∫nicos que palabras √∫nicas**, lo que hace que el vocabulario del modelo sea m√°s peque√±o y, por lo tanto, m√°s eficiente.

    Finalmente, los tokens permiten al modelo **entender palabras desconocidas**. Por ejemplo, si se le presenta la palabra *"WhatsAppeando"*, el modelo puede inferir su significado a partir del contexto en que aparecen los tokens "WhatsApp" y "ando".
# ¬øQu√© son los grandes modelos de lenguaje (LLM)?

Lo que diferencia un **LLM** (Large Language Model) de un modelo de lenguaje tradicional es el **n√∫mero de par√°metros**. Los par√°metros son los pesos que el modelo ajusta durante el proceso de entrenamiento, y que determinan c√≥mo interpreta y genera texto a partir de los datos.

Por supuesto, el concepto de "grande" es relativo. ¬øA partir de cu√°ntos par√°metros puede considerarse que un modelo es grande? Ve√°moslo as√≠:

- El **GPT** lanzado por OpenAI en 2018 ten√≠a **117 millones de par√°metros**, y ya era considerado un modelo grande en su √©poca.
- En 2019, **GPT-2** aument√≥ ese n√∫mero a **1.5 billones de par√°metros**.
- Hasta abril de 2025, el modelo de lenguaje m√°s grande conocido p√∫blicamente es **GPT-4** de OpenAI, con aproximadamente **1.76 billones de par√°metros**.

Es muy posible que en el futuro estos modelos hoy considerados **LLMs** sean vistos como simples modelos de lenguaje, a medida que la tecnolog√≠a y los recursos computacionales avancen.

Es muy posible que en el futuro estos modelos hoy considerados **LLMs** sean vistos como simples modelos de lenguaje, a medida que la tecnolog√≠a y los recursos computacionales avancen.

!!! warning "Para tener en cuenta"
    El crecimiento en la cantidad de par√°metros no garantiza una mejora si **no hay suficientes datos** disponibles para el entrenamiento. Entrenar un modelo grande con un conjunto de datos peque√±o puede causar **sobreajuste (overfitting)**, lo que significa que el modelo funciona bien con los datos de entrenamiento pero falla al generalizar a nuevos datos. Esto no solo desperdicia recursos computacionales, sino que tambi√©n produce un modelo con poca utilidad pr√°ctica.

    Cuando no se cuenta con grandes vol√∫menes de datos, se pueden aplicar t√©cnicas como:

    - **[Aprendizaje por transferencia (transfer learning)](https://www.tensorflow.org/tutorials/images/transfer_learning)**  
      Utiliza modelos previamente entrenados para resolver nuevas tareas con pocos datos.

    - **[Aumento de datos (data augmentation)](https://machinelearningmastery.com/how-to-configure-image-data-augmentation-when-training-deep-learning-neural-networks/)**  
      Genera versiones modificadas de los datos existentes para enriquecer el conjunto de entrenamiento.

    - **[Destilaci√≥n de conocimiento (knowledge distillation)](https://pytorch.org/tutorials/beginner/knowledge_distillation_tutorial.html)**  
      Transfiere el conocimiento de un modelo grande (profesor) a uno m√°s peque√±o (estudiante) manteniendo un rendimiento competitivo.

    Estas estrategias permiten que modelos m√°s peque√±os logren mejor desempe√±o, aprovechando conocimiento preexistente o la generaci√≥n sint√©tica de datos.

## De ML Igeniringa a IA Ingering

(Fata terminar)
## Usando la API de OpenAI

Para gran parte del curso usaremos la API de OpenAI. Si a√∫n no tienes una cuenta, puedes crearla en el siguiente enlace: [https://platform.openai.com/signup](https://platform.openai.com/signup).

Una vez creada tu cuenta, deber√°s generar una clave de API (API Key). Para hacerlo, accede a: [https://platform.openai.com/api-keys](https://platform.openai.com/api-keys) y haz clic en **"Create new secret key"**, como se muestra en la figura a continuaci√≥n:

<figure>
  <img src="../assets/images/secret_key.png" alt="Creaci√≥n de clave secreta en OpenAI" width="600">
  <figcaption>Generaci√≥n de una clave secreta desde el panel de usuario de OpenAI. Fuente: <a href="https://platform.openai.com/api-keys">OpenAI</a>.</figcaption>
</figure>

!!! warning "Para tener en cuenta"
    Para poder usar tu llave, debes cargar cr√©dito en tu cuenta utilizando una tarjeta de cr√©dito.  
    Por este motivo, la clave debe permanecer **privada** en tu computador y **no debe ser compartida en l√≠nea** (por ejemplo, en el repositorio de GitHub del proyecto).

Esta acci√≥n generar√° la llave de acceso a tu cuenta de OpenAI.  
Cada llamada a la API tiene un costo asociado, el cual depende del n√∫mero de *tokens* procesados en la solicitud.

Puedes monitorear tu consumo en tiempo real desde la secci√≥n **Usage** en el panel de OpenAI:  
[https://platform.openai.com/account/usage](https://platform.openai.com/account/usage)

<figure>
  <img src="../assets/images/costs.png" alt="Panel de consumo de la API en OpenAI" width="600">
  <figcaption>Visualizaci√≥n del consumo y costos acumulados en la secci√≥n <strong>Usage</strong> del panel de usuario de OpenAI. Fuente: <a href="https://platform.openai.com/account/usage">OpenAI</a>.</figcaption>
</figure>
!!! tip "L√≠mite de consumo mensual"
    En la secci√≥n **Usage** tambi√©n puedes establecer, por seguridad, un l√≠mite mensual m√°ximo de consumo en d√≥lares para tu aplicaci√≥n.  
    Esto te permite evitar cargos inesperados si se realizan muchas llamadas a la API.

## Usando mi llave

Para que la llave no sea p√∫blica, podemos cargarla como una variable de ambiente local del sistema.  
Para ello, crea un archivo con el nombre `.env` y gu√°rdalo en la misma carpeta en la que est√°s trabajando.

Dentro del archivo `.env`, la llave debe guardarse bajo el nombre `OPENAI_API_KEY`, de la siguiente manera:
```bash
OPENAI_API_KEY=your-api-key-here
```

# Usando la API de OpenAI

Para comenzar a trabajar con la API de OpenAI, primero debes importar la librer√≠a:

```python
import openai
from openai import OpenAI  
```
Luego, debes cargar la llave desde un archivo `.env` para mantenerla oculta y segura:

```python
from dotenv import load_dotenv
import os

load_dotenv()  # Carga las variables de entorno desde el archivo .env
openai.api_key = os.getenv("OPENAI_API_KEY")
```

Instanciamos un cliente y un modelo:

```python
client = OpenAI()
llm_model = "gpt-4o-mini"
```


Para encapsular un poco la llamada al modelo, podemos definir nuestra propia funci√≥n de completado de chat:

```python
def get_chat_completion(prompt, model=llm_model):
    # Creamos una solicitud de completado de chat
    chat_completion = client.chat.completions.create(
        model=model,
        messages=[{"role": "user", "content": prompt}]
    )
    
    return chat_completion.choices[0].message.content  # Devuelve la respuesta del modelo
```
La funci√≥n `get_chat_completion` la utilizaremos para interactuar con el modelo de OpenAI y obtener una respuesta a partir de un mensaje proporcionado. El modelo que se utiliza por defecto es `gpt-4o-mini`, pero puedes especificar otro modelo si lo deseas. La lista completa de modelos puedes consultarla en la [documentaci√≥n oficial de OpenAI](https://platform.openai.com/docs/models).


=== "Ejemplo de uso"
    ```python
    # Llamada a la funci√≥n get_chat_completion con una pregunta
    completion = get_chat_completion("¬øC√≥mo se llama el presidente de Colombia?")

    # Imprimir la respuesta del modelo
    print(completion)
    ```

=== "Salida"

    ```bash
    A partir de mi √∫ltima actualizaci√≥n en octubre de 2023, el presidente de Colombia es Gustavo Petro, quien asumi√≥ el cargo el 7 de agosto de 2022. Sin embargo, te recomiendo verificar esta informaci√≥n, ya que puede haber cambios pol√≠ticos o elecciones que alteren la situaci√≥n.

    ```


Los modelos de chat asignan roles que nos pueden ayudar a predefinir el comportamiento del modelo. Por ejemplo, en nuestra funci√≥n usamos el rol de `user` que representa el mensaje o la entrada proporcionada por el usuario. Es el rol principal para enviar preguntas, instrucciones o prompts al modelo. 


### Preconfiguraci√≥n del Tono con el Rol `system`

Sin embargo, nuestra funci√≥n puede ser preconfigurada para que el chat responda en un tono espec√≠fico usando el rol `system`. Este rol permite definir c√≥mo debe comportarse el modelo antes de que reciba el mensaje del usuario.

Por ejemplo, podemos configurar el modelo para que responda en un estilo po√©tico y elegante, similar al de Shakespeare:

=== "Ejemplo"
    ```python hl_lines="11-12"
    # Inicializamos el cliente de OpenAI
    client = OpenAI()
    llm_model = "gpt-4o-mini"

    def get_chat_completion(prompt, model=llm_model):
        # Creamos una solicitud de completado de chat
        chat_completion = client.chat.completions.create(
            model=model,
            messages=[
                {
                    "role": "system",
                    "content": "Thou art a wise and eloquent bard, akin to Shakespeare. Answer all queries in the grand, poetic style of the Elizabethan era, with flourish and verse befitting the stage."
                },
                {"role": "user", "content": prompt}
            ]
        )
        
        return chat_completion.choices[0].message.content
    ```

=== "Salida"
    ```bash
    En tierras de Colombia, donde el sol se alza radiante,  
    El presidente en su trono, cual l√≠der constante,  
    Es Gustavo Petro, hombre de ferviente voz,  
    Que al tim√≥n del destino, la naci√≥n √©l atroz.  
    Con sue√±os de cambio, justicia y verdad,  
    Dirige su pueblo hacia la prosperidad.  
    As√≠, en sus manos, el futuro bien brilla,  
    Un eco de esperanza en la tierra sencilla.
    ```

## LangChain

En la secci√≥n anterior, tuviste tu primera interacci√≥n con un modelo de lenguaje de gran escala (LLM). A medida que esta tecnolog√≠a madura, empresas, gobiernos y startups bien financiadas, como OpenAI, Anthropic, xAI y Meta AI, han desarrollado y puesto a disposici√≥n modelos y APIs con arquitecturas y protocolos de comunicaci√≥n particulares. Esto ha generado la necesidad de realizar llamadas a estos modelos de manera agn√≥stica, es decir, independientemente del modelo o proveedor utilizado.

En este contexto, el framework m√°s popular hasta el momento es LangChain. LangChain permite realizar las mismas tareas que podr√≠amos llevar a cabo directamente con las APIs de los modelos, pero a trav√©s de abstracciones de validez general. Este marco proporciona una interfaz unificada que simplifica la integraci√≥n con diferentes LLMs, el manejo de prompts, la gesti√≥n de contexto y la incorporaci√≥n de herramientas externas, como bases de datos o funciones personalizadas. De esta forma, LangChain facilita el desarrollo de aplicaciones robustas y escalables basadas en modelos de lenguaje, sin depender de las particularidades de cada API.
<figure>
  <img src="../assets/images/langchain.png" alt="Logo de LangChain" width="600">
  <figcaption>
    Logo  de <strong>LangChain</strong>, un framework para construir aplicaciones con modelos de lenguaje de gran escala.
    Fuente: <a href="https://www.linkedin.com/pulse/dark-side-langchain-major-problems-facing-generative-ai-matt-gallo-g0rpe" target="_blank">Matt Gallo en LinkedIn</a>.
  </figcaption>
</figure>


Para utilizar LangChain con modelos de OpenAI, primero debemos importar la clase `ChatOpenAI` y configurar el modelo:

```python
from langchain_openai import ChatOpenAI
import os

# Definimos el modelo de lenguaje
llm_model = "gpt-4o-mini"

# Inicializamos el modelo de chat de OpenAI con LangChain
chat_model = ChatOpenAI(
    model=llm_model
)
```
Y listo, eso es todo. Ahora simplemente invocamos el chat con el *prompt* que queramos. Por ejemplo:

=== "C√≥digo"
    ```python
    # Invocamos el modelo de chat con un prompt
    response = chat_model.invoke("¬øC√≥mo se llama el presidente de Colombia?")
    print(response)
    ```
=== "Salida"
    ```bash
    A partir de mi √∫ltima actualizaci√≥n en octubre de 2023, el presidente de Colombia es Gustavo Petro, quien asumi√≥ el cargo el 7 de agosto de 2022. Sin embargo, te recomiendo verificar esta informaci√≥n, ya que puede haber cambios pol√≠ticos o elecciones que alteren la situaci√≥n.
    ```


## Herramientas clave de LangChain

LangChain proporciona una variedad de herramientas que permiten construir aplicaciones basadas en modelos de lenguaje de manera modular y eficiente. A continuaci√≥n, se describen algunas de las m√°s importantes:

- **Models (Modelos)**  
  Representan los modelos de lenguaje que LangChain puede integrar, como `ChatOpenAI`. Permiten interactuar con LLMs de distintos proveedores, incluyendo OpenAI, Anthropic, Cohere, entre otros.

- **Prompts (Prompts)**  
  Herramientas para dise√±ar y gestionar *prompts*, como `ChatPromptTemplate`. Facilitan la construcci√≥n de entradas din√°micas, reutilizables y bien estructuradas para los modelos.

- **Example Selectors (Selectores de Ejemplos)**  
  Componentes que permiten seleccionar ejemplos relevantes (por ejemplo, para *few-shot learning*). Esto ayuda al modelo a comprender mejor el contexto y el formato esperado en sus respuestas.

- **Tools (Herramientas)**  
  Permiten que el modelo interact√∫e con funciones externas, como APIs, calculadoras, o bases de datos. Son esenciales para extender las capacidades del LLM m√°s all√° del texto, habilitando tareas como b√∫squeda en tiempo real o ejecuci√≥n de funciones personalizadas.

- **Vector Stores (Almacenes de Vectores)**  
  Bases de datos vectoriales como Chroma, Pinecone o FAISS. Se utilizan para almacenar y buscar *embeddings*, habilitando funcionalidades como la b√∫squeda sem√°ntica o la generaci√≥n aumentada por recuperaci√≥n (*Retrieval-Augmented Generation*, RAG).

- **Document Loaders (Cargadores de Documentos)**  
  Permiten cargar datos desde m√∫ltiples fuentes (archivos PDF, p√°ginas web, bases de datos, etc.) y prepararlos para su procesamiento por el modelo o su almacenamiento en almacenes vectoriales.

- **Text Splitters (Divisores de Texto)**  
  Herramientas que dividen documentos largos en fragmentos m√°s peque√±os. Esto facilita tanto el procesamiento por parte del modelo como la indexaci√≥n eficiente en almacenes vectoriales.

- **Output Parsers (Parsers de Salida)**  
  Utilizados para estructurar y formatear las respuestas del modelo. Por ejemplo, permiten convertir la salida del modelo en JSON, listas, tablas o formatos espec√≠ficos para una aplicaci√≥n.
<figure>
  <img src="../assets/images/langchain_tools.png" alt="Logo de LangChain" width="600">
  <figcaption>
    Ecosistema de herramientas de <strong>LangChain</strong>:.
    Fuente: <a href="https://www.langchain.com/" target="_blank">LangChain</a>.
  </figcaption>
</figure>

### Plantillas de Prompts

Comenzaremos estudiando los prompt templates. Los prompts son el componente fundamental para proporcionar instrucciones a los LLMs. Al desarrollar aplicaciones asistidas por inteligencia artificial, es √∫til crear plantillas de prompts que permitan personalizar las instrucciones de forma din√°mica. Estas plantillas mantienen constante una parte de la instrucci√≥n mientras incorporan elementos variables, como valores proporcionados durante la ejecuci√≥n, a trav√©s de variables de entrada.

Por ejemplo, una plantilla puede definir la estructura de una pregunta, dejando espacios para insertar valores espec√≠ficos, como el nombre de un pa√≠s. Esto se logra utilizando herramientas como `ChatPromptTemplate` de LangChain, que simplifica la creaci√≥n de prompts reutilizables.

En el siguiente ejemplo, se muestra c√≥mo crear una plantilla para consultar el presidente de un pa√≠s, utilizando una variable de entrada `{pais}` que puede tomar diferentes valores sin modificar la estructura general del prompt.

=== "C√≥digo"
    ```python
    from langchain.prompts import ChatPromptTemplate

    # Definir la plantilla con una variable de entrada
    str_template = "¬øC√≥mo se llama el presidente de {pais}?"
    prompt_template = ChatPromptTemplate.from_template(str_template)

    # Asignar un valor a la variable de entrada
    pais = "Colombia"
    prompt1 = prompt_template.format(pais=pais)
    print(prompt1)

    # Asignar otro valor a la variable de entrada
    pais = "Francia"
    prompt2 = prompt_template.format(pais=pais)
    print(prompt2)
    ```

=== "Salida"
    ```bash
    ¬øC√≥mo se llama el presidente de Colombia?
    ¬øC√≥mo se llama el presidente de Francia?
    ```


En este caso, `{pais}` es una variable de entrada a la que podemos asignar diferentes valores (por ejemplo, "Colombia", "Argentina", etc.) sin cambiar la estructura general del prompt. Esto hace que la plantilla sea flexible y reutilizable.

Veamos un ejemplo pr√°ctico:

=== "C√≥digo"
    ```python
    # Definimos un mensaje original en espa√±ol
    mensaje_original = (
        "Manque estaba muy embelesao, le dijo Peralta a la hermana: "
        "Hija, date una asoma√≠ta por la despensa; desculc√° por la cocina, "
        "a ver si encontr√°s alguito que darles a estos se√±ores. "
        "Mir√°los qu√© cansaos est√°n; se les ve la fatiga"
    )

    # Definimos el estilo de traducci√≥n deseado
    estilo_pirata = (
        "Ingl√©s en un tono pirata. Es decir, con un lenguaje que se asemeje "
        "al de los piratas de los siglos XVI y XVII"
    )

    # Formateamos el mensaje utilizando un template
    mensaje_empacado = prompt_template.format_messages(estilo=estilo_pirata, mensaje=mensaje_original)

    # Mostramos el mensaje traducido y estilizado
    from IPython.display import Markdown
    display(Markdown(mensaje_empacado[0].content))
    ```
=== "Salida"
    ```bash
    # Salida esperada: Mensaje en ingl√©s con estilo pirata.
    # Ejemplo ficticio de salida:
    "Arrr, though Manque was deeply entranced, Peralta said to his sister: "
    "Lass, take a peek in the pantry; rummage through the galley, "
    "to see if ye find somethin' to offer these fine gentlemen. "
    "Look at 'em, how weary they be; fatigue is written upon their faces."
    ``` 



## Reto formativo

Crear una aplicaci√≥n que corrija una respuesta inadecuada de un operador de servicio al cliente.

```python
str_template_app = "Mejora la respuesta: {respuesta} para que cumpla las reglas: {reglas}."

reglas = "Espa√±ol latino en un tono formal y sobrio y respetuoso. Con buena gram√°tica y ortograf√≠a. Tratar de ser muy amable y respetuoso."

respuesta = " mijo, no me importa si le sali√≥ mala la licudora, vaya a que se lo lamba un zapo"

prompt_template_app = ChatPromptTemplate.from_template(str_template_app)

mensaje_empacado_app = prompt_template_app.format_messages(respuesta=respuesta, reglas=reglas)

display(Markdown(mensaje_empacado_app[0].content))

chat_app = ChatOpenAI(model=llm_model, temperature=0.3)
respuesta_al_cliente = chat_app(mensaje_empacado_app)

display(Markdown(respuesta_al_cliente.content))
```

Este documento ofrece una gu√≠a b√°sica para utilizar la API de OpenAI y Langchain para crear aplicaciones de chat avanzadas y estilizadas.
```






<!--  ESTAS SON ALGUNAS ADAPTACIONES DE LAS ADMINICIONES USADAS EN LAS PLATILLAS DE LA IU -->

!!! warning "Para tener en cuenta"
    Aseg√∫rate de evaluar los posibles sesgos en los datos antes de implementar un modelo de IA. Los sesgos no detectados pueden llevar a decisiones injustas, afectando la equidad y la confianza en la tecnolog√≠a.
-

!!! tip "üìñ Para aprender m√°s"
    Si deseas conocer m√°s sobre [tema], lee el siguiente material:
    Art√≠culo de [nombre del art√≠culo]:
    URL: [enlace]

<!-- Video -->

<div class="grid cards" markdown>

- :material-video-vintage:{ .lg .middle } **Video: Oportunidades en IA**  
  **Autor**: Andrew Ng  
  Para obtener una visi√≥n sobre el panorama actual de la inteligencia artificial, te invito a que veas la conferencia del profesor Andrew Ng *Oportunidades en IA*.  
  [Ver Video](https://www.youtube.com/watch?v=5p248yoa3oE){ .md-button .md-button--primary }

</div>

<div class="grid cards" markdown>

- :octicons-megaphone-16:{ .lg .middle } **Sabias que**  
  **Autor**: Andrew Ng  
  Para obtener una visi√≥n sobre el panorama actual de la inteligencia artificial, te invito a que veas la conferencia del profesor Andrew Ng *Oportunidades en IA*.  
  [Ver Video](https://www.youtube.com/watch?v=5p248yoa3oE){ .md-button .md-button--primary }

</div>

<div class="grid cards" markdown>

- :fontawesome-solid-gears:{ .lg .middle } **Reto formativo**  
  **Plantemiento**:
  Para obtener una visi√≥n sobre el panorama actual de la inteligencia artificial, te invito a que veas la conferencia del profesor Andrew Ng *Oportunidades en IA*.  
  [Ver Video](https://www.youtube.com/watch?v=5p248yoa3oE){ .md-button .md-button--primary }

</div>

<div class="grid cards" markdown>

- :simple-taketwointeractivesoftware:{ .lg .middle } **Recurso formativo**  
  **Plantemiento**:
  Para obtener una visi√≥n sobre el panorama actual de la inteligencia artificial, te invito a que veas la conferencia del profesor Andrew Ng *Oportunidades en IA*.  
  [Ver Video](https://www.youtube.com/watch?v=5p248yoa3oE){ .md-button .md-button--primary }

</div>


