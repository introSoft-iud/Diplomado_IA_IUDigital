{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#diplomado-en-construccion-de-aplicaciones-asistidas-por-ia","title":"Diplomado en construcci\u00f3n de Aplicaciones Asistidas por IA","text":"<p>Bienvenidos al diploma en construcci\u00f3n de aplicaciones asistidas por modelos de lenguaje de la IUDigital de Antioquia</p> <p>Aunque la inteligencia artificial ha existido como un campo exitoso y prometedor entre los expertos durante varias d\u00e9cadas, la llegada de capacidades computacionales m\u00e1s avanzadas \u2014ofrecidas por las GPU modernas\u2014 y las habilidades demostradas con el lanzamiento de ChatGPT fueron una gran sorpresa para muchos.</p> <p>No est\u00e1 del todo claro c\u00f3mo este \"peque\u00f1o\" avance en la escala de los modelos pudo desencadenar la gran cantidad de aplicaciones asistidas por IA que estamos viendo explotar cada semana. Lo que s\u00ed es claro es que el campo de la ingenier\u00eda de software est\u00e1 siendo revolucionado, y que el nuevo paradigma de construcci\u00f3n de software ya no consiste en los tradicionales flujos de ejecuci\u00f3n, sino que la nueva ingenier\u00eda de sistemas debe integrar a los LLMs en dichos flujos.</p> <p>Hemos dise\u00f1ado este diplomado para introducirte a este nuevo paradigma. </p>"},{"location":"#resultados-de-aprendizage","title":"Resultados de Aprendizage","text":""},{"location":"#pregunta-orientadora","title":"Pregunta Orientadora","text":""},{"location":"#mapa-del-curso","title":"Mapa del curso","text":"<p>crea a aqu\u00ed el mapa del curso</p>"},{"location":"#cronograma-de-actividades","title":"Cronograma de actividades","text":"Actividad de aprendizaje Evidencia de aprendizaje Semana Ponderaci\u00f3n Caracter\u00edsticas principales del reino Fungi Actividad de conocimientos previos Semana 1 10 % Principios de la teor\u00eda del aprendizaje Mapa mental sobre teor\u00eda del aprendizaje Semana 2 20 % Agrega aqu\u00ed otra actividad si aplica Agrega aqu\u00ed otra evidencia Semana % Agrega aqu\u00ed otra actividad si aplica Agrega aqu\u00ed otra evidencia Semana % Agrega aqu\u00ed otra actividad si aplica Agrega aqu\u00ed otra evidencia Semana % Total 100 %"},{"location":"#actividad-de-refuerzo","title":"Actividad de refuerzo","text":"<p>Agrega el nombre de la actividad de refuerzo (cuando as\u00ed se requiera).</p>"},{"location":"#configuracion-del-sistema-antes-de-comenzar","title":"Configuraci\u00f3n del sistema antes de comenzar","text":"<p>Antes de empezar a trabajar con los m\u00f3dulos del curso, debes configurar tu sistema para poder ejecutar los ejemplos correctamente.</p> <p>La forma m\u00e1s sencilla de hacerlo es descargando el archivo de configuraci\u00f3n \ud83d\udcc4environment.yml , el cual crear\u00e1 autom\u00e1ticamente un entorno de Conda llamado <code>DiplomadoIA_env</code> con todas las dependencias necesarias para el curso.</p>"},{"location":"#requisitos-previos","title":"Requisitos previos","text":"<ul> <li>Tener Anaconda instalado en tu computador.</li> <li>Usar una terminal Bash (en Windows puedes usar Anaconda Prompt, git bash, WSL o similares).</li> </ul>"},{"location":"#instalacion","title":"Instalaci\u00f3n","text":"<p>Una vez descargado el archivo de configuraci\u00f3n, ejecuta el siguiente comando en tu terminal:</p> <pre><code>conda env create -f environment.yml\n</code></pre>"},{"location":"#activacion-del-entorno","title":"Activaci\u00f3n del entorno","text":"<p>Para activar el entorno en tu terminal, ejecuta:</p> <p><pre><code>conda activate diplomado_IA\n</code></pre> A partir de aqu\u00ed, cualquier comando que ejecutes usar\u00e1 las dependencias definidas para el curso.</p> <p>Uso del entorno en Visual Studio Code</p> <p>Para tener en cuenta</p> <p>Para ejecutar notebooks <code>.ipynb</code> en Visual Studio Code usando este entorno:</p> <ol> <li>Abre VS Code.</li> <li>Abre la carpeta del proyecto o el notebook deseado.</li> <li>En la parte superior derecha del notebook, haz clic en la selecci\u00f3n de kernel.</li> <li>Elige el kernel correspondiente al entorno <code>diplomado_IA</code>. Si no aparece, reinicia VS Code o aseg\u00farate de haber activado el entorno desde la terminal integrada.</li> <li>Comienza a ejecutar celdas normalmente.</li> </ol> <p>Tip</p> <p>Puedes asegurarte de que el entorno se registre correctamente como kernel ejecutando en la terminal: <pre><code>python -m ipykernel install --user --name diplomado_IA --display-name \"Python (diplomado_IA)\"\n</code></pre></p>"},{"location":"Unidad%201/","title":"Index","text":""},{"location":"Unidad%201/#unidad-1-introduccion-a-la-construccion-de-aplicaciones-con-llms","title":"Unidad 1. Introducci\u00f3n a la construcci\u00f3n de aplicaciones con LLMs","text":""},{"location":"Unidad%201/#introduccion-a-la-unidad","title":"Introducci\u00f3n a la unidad","text":"<p>Bienvenidos a la primera unidad. En esta unidad aprender\u00e1s, de manera general, c\u00f3mo funciona un modelo de lenguaje. Comenzaremos usando la API de OpenAI y exploraremos c\u00f3mo conectar sus servicios con nuestras aplicaciones. Luego aprender\u00e1s a utilizar esta misma API a trav\u00e9s del framework LangChain.</p> <p>Finalmente, introduciremos los aspectos fundamentales de la interacci\u00f3n con los LLMs usando LangChain: prompts, templates y output parsers.</p> <p>Como actividad pr\u00e1ctica, elaborar\u00e1s un sistema asistido por IA para extraer datos de comentarios de usuarios en un e-commerce.</p> <p>\u00a1Comencemos!</p>"},{"location":"Unidad%201/#resultados-de-aprendizaje","title":"Resultados de aprendizaje","text":""},{"location":"Unidad%201/#cronograma-de-actividades-de-la-unidad","title":"Cronograma de actividades de la unidad","text":"<p>**</p>"},{"location":"Unidad%201/#cronograma-de-actividades-unidad-1","title":"Cronograma de actividades - Unidad 1","text":"Actividad de aprendizaje Evidencia de aprendizaje Semana Ponderaci\u00f3n Actividades de aprendizaje 1 y 2 EA1 Semana 1 y 2 25% Total 25 % **"},{"location":"Unidad%201/#que-es-un-modelo-de-lenguaje","title":"\u00bfQu\u00e9 es un modelo de lenguaje?","text":"<p>Un modelo de lenguaje es un sistema basado en deep learning que encapsula informaci\u00f3n sobre uno o varios lenguajes. Este sistema es entrenado para predecir qu\u00e9 tan probable es que una palabra aparezca en un determinado contexto.</p> <p>Por ejemplo, dado el contexto:</p> <p>\"Mi plato favorito es el ____\"</p> <p>un modelo de lenguaje que codifique el espa\u00f1ol de Antioquia podr\u00eda predecir \"sancocho\" con m\u00e1s frecuencia que \"ajiaco\".</p>"},{"location":"Unidad%201/#tokens","title":"Tokens","text":"<p>La unidad b\u00e1sica de predicci\u00f3n de un modelo de lenguaje es el token, y el tokenizador es el software que utiliza el modelo para dividir los textos en tokens.</p> <p>Por ejemplo, el tokenizador de GPT-4 divide la frase:</p> <p>\"El sol est\u00e1 brillando intensamente\"</p> <p>de la siguiente manera:</p> Divisi\u00f3n en tokens de una frase utilizando el tokenizador de GPT-4. Fuente: OpenAI Tokenizer. <p>Para tener en cuenta</p> <p>Hay varias razones por las que los modelos de lenguaje utilizan tokens en lugar de palabras completas o caracteres individuales.</p> <p>A diferencia de un simple car\u00e1cter, un token permite dividir una palabra en componentes con significado propio. Por ejemplo, la palabra \"intensamente\" puede ser dividida por el tokenizador en \"intens\" y \"amente\", y cada uno de estos componentes aporta parte del significado de la palabra completa.</p> <p>Esto tambi\u00e9n implica que hay menos tokens \u00fanicos que palabras \u00fanicas, lo que hace que el vocabulario del modelo sea m\u00e1s peque\u00f1o y, por lo tanto, m\u00e1s eficiente.</p> <p>Finalmente, los tokens permiten al modelo entender palabras desconocidas. Por ejemplo, si se le presenta la palabra \"WhatsAppeando\", el modelo puede inferir su significado a partir del contexto en que aparecen los tokens \"WhatsApp\" y \"ando\".</p>"},{"location":"Unidad%201/#que-son-los-grandes-modelos-de-lenguaje-llm","title":"\u00bfQu\u00e9 son los grandes modelos de lenguaje (LLM)?","text":"<p>Lo que diferencia un LLM (Large Language Model) de un modelo de lenguaje tradicional es el n\u00famero de par\u00e1metros. Los par\u00e1metros son los pesos que el modelo ajusta durante el proceso de entrenamiento, y que determinan c\u00f3mo interpreta y genera texto a partir de los datos.</p> <p>Por supuesto, el concepto de \"grande\" es relativo. \u00bfA partir de cu\u00e1ntos par\u00e1metros puede considerarse que un modelo es grande? Ve\u00e1moslo as\u00ed:</p> <ul> <li>El GPT lanzado por OpenAI en 2018 ten\u00eda 117 millones de par\u00e1metros, y ya era considerado un modelo grande en su \u00e9poca.</li> <li>En 2019, GPT-2 aument\u00f3 ese n\u00famero a 1.5 billones de par\u00e1metros.</li> <li>Hasta abril de 2025, el modelo de lenguaje m\u00e1s grande conocido p\u00fablicamente es GPT-4 de OpenAI, con aproximadamente 1.76 billones de par\u00e1metros.</li> </ul> <p>Es muy posible que en el futuro estos modelos hoy considerados LLMs sean vistos como simples modelos de lenguaje, a medida que la tecnolog\u00eda y los recursos computacionales avancen.</p> <p>Es muy posible que en el futuro estos modelos hoy considerados LLMs sean vistos como simples modelos de lenguaje, a medida que la tecnolog\u00eda y los recursos computacionales avancen.</p> <p>Para tener en cuenta</p> <p>El crecimiento en la cantidad de par\u00e1metros no garantiza una mejora si no hay suficientes datos disponibles para el entrenamiento. Entrenar un modelo grande con un conjunto de datos peque\u00f1o puede causar sobreajuste (overfitting), lo que significa que el modelo funciona bien con los datos de entrenamiento pero falla al generalizar a nuevos datos. Esto no solo desperdicia recursos computacionales, sino que tambi\u00e9n produce un modelo con poca utilidad pr\u00e1ctica.</p> <p>Cuando no se cuenta con grandes vol\u00famenes de datos, se pueden aplicar t\u00e9cnicas como:</p> <ul> <li> <p>Aprendizaje por transferencia (transfer learning)   Utiliza modelos previamente entrenados para resolver nuevas tareas con pocos datos.</p> </li> <li> <p>Aumento de datos (data augmentation)   Genera versiones modificadas de los datos existentes para enriquecer el conjunto de entrenamiento.</p> </li> <li> <p>Destilaci\u00f3n de conocimiento (knowledge distillation)   Transfiere el conocimiento de un modelo grande (profesor) a uno m\u00e1s peque\u00f1o (estudiante) manteniendo un rendimiento competitivo.</p> </li> </ul> <p>Estas estrategias permiten que modelos m\u00e1s peque\u00f1os logren mejor desempe\u00f1o, aprovechando conocimiento preexistente o la generaci\u00f3n sint\u00e9tica de datos.</p>"},{"location":"Unidad%201/#de-ml-igeniringa-a-ia-ingering","title":"De ML Igeniringa a IA Ingering","text":"<p>(Fata terminar)</p>"},{"location":"Unidad%201/#usando-la-api-de-openai","title":"Usando la API de OpenAI","text":"<p>Para gran parte del curso usaremos la API de OpenAI. Si a\u00fan no tienes una cuenta, puedes crearla en el siguiente enlace: https://platform.openai.com/signup.</p> <p>Una vez creada tu cuenta, deber\u00e1s generar una clave de API (API Key). Para hacerlo, accede a: https://platform.openai.com/api-keys y haz clic en \"Create new secret key\", como se muestra en la figura a continuaci\u00f3n:</p> Generaci\u00f3n de una clave secreta desde el panel de usuario de OpenAI. Fuente: OpenAI. <p>Para tener en cuenta</p> <p>Para poder usar tu llave, debes cargar cr\u00e9dito en tu cuenta utilizando una tarjeta de cr\u00e9dito. Por este motivo, la clave debe permanecer privada en tu computador y no debe ser compartida en l\u00ednea (por ejemplo, en el repositorio de GitHub del proyecto).</p> <p>Esta acci\u00f3n generar\u00e1 la llave de acceso a tu cuenta de OpenAI. Cada llamada a la API tiene un costo asociado, el cual depende del n\u00famero de tokens procesados en la solicitud.</p> <p>Puedes monitorear tu consumo en tiempo real desde la secci\u00f3n Usage en el panel de OpenAI: https://platform.openai.com/account/usage</p> Visualizaci\u00f3n del consumo y costos acumulados en la secci\u00f3n Usage del panel de usuario de OpenAI. Fuente: OpenAI. <p>L\u00edmite de consumo mensual</p> <p>En la secci\u00f3n Usage tambi\u00e9n puedes establecer, por seguridad, un l\u00edmite mensual m\u00e1ximo de consumo en d\u00f3lares para tu aplicaci\u00f3n. Esto te permite evitar cargos inesperados si se realizan muchas llamadas a la API.</p>"},{"location":"Unidad%201/#usando-mi-llave","title":"Usando mi llave","text":"<p>Para que la llave no sea p\u00fablica, podemos cargarla como una variable de ambiente local del sistema. Para ello, crea un archivo con el nombre <code>.env</code> y gu\u00e1rdalo en la misma carpeta en la que est\u00e1s trabajando.</p> <p>Dentro del archivo <code>.env</code>, la llave debe guardarse bajo el nombre <code>OPENAI_API_KEY</code>, de la siguiente manera: <pre><code>OPENAI_API_KEY=your-api-key-here\n</code></pre></p>"},{"location":"Unidad%201/#usando-la-api-de-openai_1","title":"Usando la API de OpenAI","text":"<p>Para comenzar a trabajar con la API de OpenAI, primero debes importar la librer\u00eda:</p> <p><pre><code>import openai\nfrom openai import OpenAI  \n</code></pre> Luego, debes cargar la llave desde un archivo <code>.env</code> para mantenerla oculta y segura:</p> <pre><code>from dotenv import load_dotenv\nimport os\n\nload_dotenv()  # Carga las variables de entorno desde el archivo .env\nopenai.api_key = os.getenv(\"OPENAI_API_KEY\")\n</code></pre> <p>Instanciamos un cliente y un modelo:</p> <pre><code>client = OpenAI()\nllm_model = \"gpt-4o-mini\"\n</code></pre> <p>Para encapsular un poco la llamada al modelo, podemos definir nuestra propia funci\u00f3n de completado de chat:</p> <p><pre><code>def get_chat_completion(prompt, model=llm_model):\n    # Creamos una solicitud de completado de chat\n    chat_completion = client.chat.completions.create(\n        model=model,\n        messages=[{\"role\": \"user\", \"content\": prompt}]\n    )\n\n    return chat_completion.choices[0].message.content  # Devuelve la respuesta del modelo\n</code></pre> La funci\u00f3n <code>get_chat_completion</code> la utilizaremos para interactuar con el modelo de OpenAI y obtener una respuesta a partir de un mensaje proporcionado. El modelo que se utiliza por defecto es <code>gpt-4o-mini</code>, pero puedes especificar otro modelo si lo deseas. La lista completa de modelos puedes consultarla en la documentaci\u00f3n oficial de OpenAI.</p> Ejemplo de usoSalida <pre><code># Llamada a la funci\u00f3n get_chat_completion con una pregunta\ncompletion = get_chat_completion(\"\u00bfC\u00f3mo se llama el presidente de Colombia?\")\n\n# Imprimir la respuesta del modelo\nprint(completion)\n</code></pre> <pre><code>A partir de mi \u00faltima actualizaci\u00f3n en octubre de 2023, el presidente de Colombia es Gustavo Petro, quien asumi\u00f3 el cargo el 7 de agosto de 2022. Sin embargo, te recomiendo verificar esta informaci\u00f3n, ya que puede haber cambios pol\u00edticos o elecciones que alteren la situaci\u00f3n.\n</code></pre> <p>Los modelos de chat asignan roles que nos pueden ayudar a predefinir el comportamiento del modelo. Por ejemplo, en nuestra funci\u00f3n usamos el rol de <code>user</code> que representa el mensaje o la entrada proporcionada por el usuario. Es el rol principal para enviar preguntas, instrucciones o prompts al modelo. </p>"},{"location":"Unidad%201/#preconfiguracion-del-tono-con-el-rol-system","title":"Preconfiguraci\u00f3n del Tono con el Rol <code>system</code>","text":"<p>Sin embargo, nuestra funci\u00f3n puede ser preconfigurada para que el chat responda en un tono espec\u00edfico usando el rol <code>system</code>. Este rol permite definir c\u00f3mo debe comportarse el modelo antes de que reciba el mensaje del usuario.</p> <p>Por ejemplo, podemos configurar el modelo para que responda en un estilo po\u00e9tico y elegante, similar al de Shakespeare:</p> EjemploSalida <pre><code># Inicializamos el cliente de OpenAI\nclient = OpenAI()\nllm_model = \"gpt-4o-mini\"\n\ndef get_chat_completion(prompt, model=llm_model):\n    # Creamos una solicitud de completado de chat\n    chat_completion = client.chat.completions.create(\n        model=model,\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": \"Thou art a wise and eloquent bard, akin to Shakespeare. Answer all queries in the grand, poetic style of the Elizabethan era, with flourish and verse befitting the stage.\"\n            },\n            {\"role\": \"user\", \"content\": prompt}\n        ]\n    )\n\n    return chat_completion.choices[0].message.content\n</code></pre> <pre><code>En tierras de Colombia, donde el sol se alza radiante,  \nEl presidente en su trono, cual l\u00edder constante,  \nEs Gustavo Petro, hombre de ferviente voz,  \nQue al tim\u00f3n del destino, la naci\u00f3n \u00e9l atroz.  \nCon sue\u00f1os de cambio, justicia y verdad,  \nDirige su pueblo hacia la prosperidad.  \nAs\u00ed, en sus manos, el futuro bien brilla,  \nUn eco de esperanza en la tierra sencilla.\n</code></pre>"},{"location":"Unidad%201/#langchain","title":"LangChain","text":"<p>En la secci\u00f3n anterior, tuviste tu primera interacci\u00f3n con un modelo de lenguaje de gran escala (LLM). A medida que esta tecnolog\u00eda madura, empresas, gobiernos y startups bien financiadas, como OpenAI, Anthropic, xAI y Meta AI, han desarrollado y puesto a disposici\u00f3n modelos y APIs con arquitecturas y protocolos de comunicaci\u00f3n particulares. Esto ha generado la necesidad de realizar llamadas a estos modelos de manera agn\u00f3stica, es decir, independientemente del modelo o proveedor utilizado.</p> <p>En este contexto, el framework m\u00e1s popular hasta el momento es LangChain. LangChain permite realizar las mismas tareas que podr\u00edamos llevar a cabo directamente con las APIs de los modelos, pero a trav\u00e9s de abstracciones de validez general. Este marco proporciona una interfaz unificada que simplifica la integraci\u00f3n con diferentes LLMs, el manejo de prompts, la gesti\u00f3n de contexto y la incorporaci\u00f3n de herramientas externas, como bases de datos o funciones personalizadas. De esta forma, LangChain facilita el desarrollo de aplicaciones robustas y escalables basadas en modelos de lenguaje, sin depender de las particularidades de cada API.</p>      Logo  de LangChain, un framework para construir aplicaciones con modelos de lenguaje de gran escala.     Fuente: Matt Gallo en LinkedIn.    <p>Para utilizar LangChain con modelos de OpenAI, primero debemos importar la clase <code>ChatOpenAI</code> y configurar el modelo:</p> <p><pre><code>from langchain_openai import ChatOpenAI\nimport os\n\n# Definimos el modelo de lenguaje\nllm_model = \"gpt-4o-mini\"\n\n# Inicializamos el modelo de chat de OpenAI con LangChain\nchat_model = ChatOpenAI(\n    model=llm_model\n)\n</code></pre> Y listo, eso es todo. Ahora simplemente invocamos el chat con el prompt que queramos. Por ejemplo:</p> C\u00f3digoSalida <pre><code># Invocamos el modelo de chat con un prompt\nresponse = chat_model.invoke(\"\u00bfC\u00f3mo se llama el presidente de Colombia?\")\nprint(response)\n</code></pre> <pre><code>A partir de mi \u00faltima actualizaci\u00f3n en octubre de 2023, el presidente de Colombia es Gustavo Petro, quien asumi\u00f3 el cargo el 7 de agosto de 2022. Sin embargo, te recomiendo verificar esta informaci\u00f3n, ya que puede haber cambios pol\u00edticos o elecciones que alteren la situaci\u00f3n.\n</code></pre>"},{"location":"Unidad%201/#herramientas-clave-de-langchain","title":"Herramientas clave de LangChain","text":"<p>LangChain proporciona una variedad de herramientas que permiten construir aplicaciones basadas en modelos de lenguaje de manera modular y eficiente. A continuaci\u00f3n, se describen algunas de las m\u00e1s importantes:</p> <ul> <li> <p>Models (Modelos)   Representan los modelos de lenguaje que LangChain puede integrar, como <code>ChatOpenAI</code>. Permiten interactuar con LLMs de distintos proveedores, incluyendo OpenAI, Anthropic, Cohere, entre otros.</p> </li> <li> <p>Prompts (Prompts)   Herramientas para dise\u00f1ar y gestionar prompts, como <code>ChatPromptTemplate</code>. Facilitan la construcci\u00f3n de entradas din\u00e1micas, reutilizables y bien estructuradas para los modelos.</p> </li> <li> <p>Example Selectors (Selectores de Ejemplos)   Componentes que permiten seleccionar ejemplos relevantes (por ejemplo, para few-shot learning). Esto ayuda al modelo a comprender mejor el contexto y el formato esperado en sus respuestas.</p> </li> <li> <p>Tools (Herramientas)   Permiten que el modelo interact\u00fae con funciones externas, como APIs, calculadoras, o bases de datos. Son esenciales para extender las capacidades del LLM m\u00e1s all\u00e1 del texto, habilitando tareas como b\u00fasqueda en tiempo real o ejecuci\u00f3n de funciones personalizadas.</p> </li> <li> <p>Vector Stores (Almacenes de Vectores)   Bases de datos vectoriales como Chroma, Pinecone o FAISS. Se utilizan para almacenar y buscar embeddings, habilitando funcionalidades como la b\u00fasqueda sem\u00e1ntica o la generaci\u00f3n aumentada por recuperaci\u00f3n (Retrieval-Augmented Generation, RAG).</p> </li> <li> <p>Document Loaders (Cargadores de Documentos)   Permiten cargar datos desde m\u00faltiples fuentes (archivos PDF, p\u00e1ginas web, bases de datos, etc.) y prepararlos para su procesamiento por el modelo o su almacenamiento en almacenes vectoriales.</p> </li> <li> <p>Text Splitters (Divisores de Texto)   Herramientas que dividen documentos largos en fragmentos m\u00e1s peque\u00f1os. Esto facilita tanto el procesamiento por parte del modelo como la indexaci\u00f3n eficiente en almacenes vectoriales.</p> </li> <li> <p>Output Parsers (Parsers de Salida)   Utilizados para estructurar y formatear las respuestas del modelo. Por ejemplo, permiten convertir la salida del modelo en JSON, listas, tablas o formatos espec\u00edficos para una aplicaci\u00f3n.</p> </li> </ul>      Ecosistema de herramientas de LangChain:.     Fuente: LangChain."},{"location":"Unidad%201/#plantillas-de-prompts","title":"Plantillas de Prompts","text":"<p>Comenzaremos estudiando los prompt templates. Los prompts son el componente fundamental para proporcionar instrucciones a los LLMs. Al desarrollar aplicaciones asistidas por inteligencia artificial, es \u00fatil crear plantillas de prompts que permitan personalizar las instrucciones de forma din\u00e1mica. Estas plantillas mantienen constante una parte de la instrucci\u00f3n mientras incorporan elementos variables, como valores proporcionados durante la ejecuci\u00f3n, a trav\u00e9s de variables de entrada.</p> <p>Por ejemplo, una plantilla puede definir la estructura de una pregunta, dejando espacios para insertar valores espec\u00edficos, como el nombre de un pa\u00eds. Esto se logra utilizando herramientas como <code>ChatPromptTemplate</code> de LangChain, que simplifica la creaci\u00f3n de prompts reutilizables.</p> <p>En el siguiente ejemplo, se muestra c\u00f3mo crear una plantilla para consultar el presidente de un pa\u00eds, utilizando una variable de entrada <code>{pais}</code> que puede tomar diferentes valores sin modificar la estructura general del prompt.</p> C\u00f3digoSalida <pre><code>from langchain.prompts import ChatPromptTemplate\n\n# Definir la plantilla con una variable de entrada\nstr_template = \"\u00bfC\u00f3mo se llama el presidente de {pais}?\"\nprompt_template = ChatPromptTemplate.from_template(str_template)\n\n# Asignar un valor a la variable de entrada\npais = \"Colombia\"\nprompt1 = prompt_template.format(pais=pais)\nprint(prompt1)\n\n# Asignar otro valor a la variable de entrada\npais = \"Francia\"\nprompt2 = prompt_template.format(pais=pais)\nprint(prompt2)\n</code></pre> <pre><code>\u00bfC\u00f3mo se llama el presidente de Colombia?\n\u00bfC\u00f3mo se llama el presidente de Francia?\n</code></pre> <p>En este caso, <code>{pais}</code> es una variable de entrada a la que podemos asignar diferentes valores (por ejemplo, \"Colombia\", \"Argentina\", etc.) sin cambiar la estructura general del prompt. Esto hace que la plantilla sea flexible y reutilizable.</p> <p>Veamos un ejemplo pr\u00e1ctico:</p> C\u00f3digoSalida <pre><code># Definimos un mensaje original en espa\u00f1ol\nmensaje_original = (\n    \"Manque estaba muy embelesao, le dijo Peralta a la hermana: \"\n    \"Hija, date una asoma\u00edta por la despensa; desculc\u00e1 por la cocina, \"\n    \"a ver si encontr\u00e1s alguito que darles a estos se\u00f1ores. \"\n    \"Mir\u00e1los qu\u00e9 cansaos est\u00e1n; se les ve la fatiga\"\n)\n\n# Definimos el estilo de traducci\u00f3n deseado\nestilo_pirata = (\n    \"Ingl\u00e9s en un tono pirata. Es decir, con un lenguaje que se asemeje \"\n    \"al de los piratas de los siglos XVI y XVII\"\n)\n\n# Formateamos el mensaje utilizando un template\nmensaje_empacado = prompt_template.format_messages(estilo=estilo_pirata, mensaje=mensaje_original)\n\n# Mostramos el mensaje traducido y estilizado\nfrom IPython.display import Markdown\ndisplay(Markdown(mensaje_empacado[0].content))\n</code></pre> <pre><code># Salida esperada: Mensaje en ingl\u00e9s con estilo pirata.\n# Ejemplo ficticio de salida:\n\"Arrr, though Manque was deeply entranced, Peralta said to his sister: \"\n\"Lass, take a peek in the pantry; rummage through the galley, \"\n\"to see if ye find somethin' to offer these fine gentlemen. \"\n\"Look at 'em, how weary they be; fatigue is written upon their faces.\"\n</code></pre>"},{"location":"Unidad%201/#ejercicio-practico","title":"Ejercicio Pr\u00e1ctico","text":"<p>Crear una aplicaci\u00f3n que corrija una respuesta inadecuada de un operador de servicio al cliente.</p> <pre><code>str_template_app = \"Mejora la respuesta: {respuesta} para que cumpla las reglas: {reglas}.\"\n\nreglas = \"Espa\u00f1ol latino en un tono formal y sobrio y respetuoso. Con buena gram\u00e1tica y ortograf\u00eda. Tratar de ser muy amable y respetuoso.\"\n\nrespuesta = \" mijo, no me importa si le sali\u00f3 mala la licudora, vaya a que se lo lamba un zapo\"\n\nprompt_template_app = ChatPromptTemplate.from_template(str_template_app)\n\nmensaje_empacado_app = prompt_template_app.format_messages(respuesta=respuesta, reglas=reglas)\n\ndisplay(Markdown(mensaje_empacado_app[0].content))\n\nchat_app = ChatOpenAI(model=llm_model, temperature=0.3)\nrespuesta_al_cliente = chat_app(mensaje_empacado_app)\n\ndisplay(Markdown(respuesta_al_cliente.content))\n</code></pre> <p>Este documento ofrece una gu\u00eda b\u00e1sica para utilizar la API de OpenAI y Langchain para crear aplicaciones de chat avanzadas y estilizadas. ```</p> <p>Para tener en cuenta</p> <p>Aseg\u00farate de evaluar los posibles sesgos en los datos antes de implementar un modelo de IA. Los sesgos no detectados pueden llevar a decisiones injustas, afectando la equidad y la confianza en la tecnolog\u00eda.</p> <p>-</p> <p>\ud83d\udcd6 Para aprender m\u00e1s</p> <p>Si deseas conocer m\u00e1s sobre [tema], lee el siguiente material: Art\u00edculo de [nombre del art\u00edculo]: URL: [enlace]</p> <ul> <li> Video: Oportunidades en IA Autor: Andrew Ng   Para obtener una visi\u00f3n sobre el panorama actual de la inteligencia artificial, te invito a que veas la conferencia del profesor Andrew Ng Oportunidades en IA. Ver Video</li> </ul> <ul> <li> Sabias que Autor: Andrew Ng   Para obtener una visi\u00f3n sobre el panorama actual de la inteligencia artificial, te invito a que veas la conferencia del profesor Andrew Ng Oportunidades en IA. Ver Video</li> </ul> <ul> <li> Reto formativo Plantemiento:   Para obtener una visi\u00f3n sobre el panorama actual de la inteligencia artificial, te invito a que veas la conferencia del profesor Andrew Ng Oportunidades en IA. Ver Video</li> </ul> <ul> <li> Recurso formativo Plantemiento:   Para obtener una visi\u00f3n sobre el panorama actual de la inteligencia artificial, te invito a que veas la conferencia del profesor Andrew Ng Oportunidades en IA. Ver Video</li> </ul>"}]}